# =============================================================================
# LLM_Utilities - Environment Configuration
# =============================================================================
#
# Copy this file to .env and modify as needed:
#   cp .env.example .env
#
# All values shown are the defaults. Uncomment and change only what you need.
# Module-specific variables (e.g., CHUNKING_*) override the global equivalents.
#

# =============================================================================
# Core LLM Backend
# =============================================================================

# LLM backend: ollama | vllm
#LLM_BACKEND=ollama

# Ollama API URL
#OLLAMA_URL=http://localhost:11434

# VLLM API URL
#VLLM_URL=http://localhost:8000

# Default model for all tasks
#DEFAULT_MODEL=gemma3:4b

# Default temperature for LLM generation
#LLM_TEMPERATURE=0.3

# =============================================================================
# LLM Connection Settings
# =============================================================================

# Connection timeout in seconds
#LLM_CONNECTION_TIMEOUT=300

# HTTP connection pool limit
#LLM_CONNECTION_POOL_LIMIT=50

# HTTP connection pool limit per host
#LLM_CONNECTION_POOL_LIMIT_PER_HOST=50

# =============================================================================
# Redis Configuration
# =============================================================================

#REDIS_HOST=localhost
#REDIS_PORT=6379
#REDIS_DB=0

# Session TTL in seconds (default: 2 hours)
#REFINEMENT_TTL=7200

# =============================================================================
# Text Extractor
# =============================================================================

# Temporary directory for uploaded files
#EXTRACTOR_UPLOAD_DIR=/tmp/text_extractor/uploads

# Temporary directory for extracted images
#EXTRACTOR_IMAGE_DIR=/tmp/text_extractor/images

# Maximum file size in MB
#EXTRACTOR_MAX_FILE_SIZE_MB=50

# Maximum pages to process per document
#EXTRACTOR_MAX_PAGES=50

# =============================================================================
# Chunking Module
# =============================================================================

# Module-specific overrides (fall back to global LLM settings if unset)
#CHUNKING_LLM_BACKEND=ollama
#CHUNKING_OLLAMA_URL=http://localhost:11434
#CHUNKING_VLLM_URL=http://localhost:8000
#CHUNKING_DEFAULT_MODEL=gemma3:4b

# Connection settings
#CHUNKING_CONNECTION_TIMEOUT=300
#CHUNKING_CONNECTION_POOL_LIMIT=50

# Chunk overlap in characters
#CHUNKING_DEFAULT_OVERLAP=200

# Tokens reserved for prompt in each chunk
#CHUNKING_DEFAULT_RESERVE_FOR_PROMPT=1000

# Enable image processing during chunking (true/false)
#CHUNKING_DEFAULT_PROCESS_IMAGES=true

# Minimum text length to create a chunk
#CHUNKING_MIN_TEXT_LENGTH=10

# Default context length for unknown models
#CHUNKING_DEFAULT_CONTEXT_LENGTH=8192

# Characters per token estimate
#CHUNKING_CHARS_PER_TOKEN=4

# Maximum chunks per batch
#CHUNKING_MAX_BATCH_SIZE=100

# =============================================================================
# Vision Processor (used by chunking for image descriptions)
# =============================================================================

# Vision LLM backend: ollama | vllm (falls back to CHUNKING_LLM_BACKEND, then LLM_BACKEND)
#VISION_LLM_BACKEND=ollama

# Vision model name
#VISION_MODEL=gemma3:4b

# Ollama URL for vision (falls back to OLLAMA_URL)
#VISION_OLLAMA_URL=http://localhost:11434

# VLLM URL for vision (falls back to CHUNKING_VLLM_URL, then VLLM_URL)
#VISION_VLLM_URL=http://localhost:8000

# Request timeout in seconds
#VISION_REQUEST_TIMEOUT=120

# Maximum concurrent vision requests (default: 2, safe for CPU; bump to 3-5 on GPU)
#VISION_MAX_CONCURRENT=2

# Vision model temperature
#VISION_TEMPERATURE=0.3

# Max tokens for vision output (500 is safe for scanned PDFs; lower to 200 for simple images)
#VISION_MAX_TOKENS=500

# =============================================================================
# Summarization Module
# =============================================================================

# Module-specific overrides (fall back to global LLM settings if unset)
#SUMMARIZATION_LLM_BACKEND=ollama
#SUMMARIZATION_OLLAMA_URL=http://localhost:11434
#SUMMARIZATION_VLLM_URL=http://localhost:8000
#SUMMARIZATION_DEFAULT_MODEL=gemma3:4b

# Default summary type: brief | detailed | bulletwise
#SUMMARIZATION_DEFAULT_TYPE=detailed

# Batch processing limits
#SUMMARIZATION_MAX_WORDS_PER_BATCH=3000
#SUMMARIZATION_MAX_CHUNKS_PER_BATCH=10

# Target word counts for summaries
#SUMMARIZATION_INTERMEDIATE_WORDS=300
#SUMMARIZATION_FINAL_WORDS=500

# LLM generation settings
#SUMMARIZATION_TEMPERATURE=0.3
#SUMMARIZATION_MAX_TOKENS=2048

# Concurrency: max parallel LLM calls during MAP phase (default: 2, safe for CPU; bump to 3-5 on GPU)
#SUMMARIZATION_MAP_CONCURRENT=2

# Concurrency: max parallel LLM calls during REDUCE phase (default: 2)
#SUMMARIZATION_REDUCE_CONCURRENT=2

# Connection settings
#SUMMARIZATION_CONNECTION_TIMEOUT=300
#SUMMARIZATION_CONNECTION_POOL_LIMIT=50

# Max input tokens as percentage of model context
#SUMMARIZATION_MAX_TOKEN_PERCENT=80

# =============================================================================
# Translation Module
# =============================================================================

# Module-specific overrides (fall back to global LLM settings if unset)
#TRANSLATION_LLM_BACKEND=ollama
#TRANSLATION_OLLAMA_URL=http://localhost:11434
#TRANSLATION_VLLM_URL=http://localhost:8000
#TRANSLATION_DEFAULT_MODEL=gemma3:4b

# Default target language
#TRANSLATION_DEFAULT_TARGET_LANGUAGE=english

# Maximum texts per batch request
#TRANSLATION_MAX_BATCH_SIZE=50

# LLM generation settings
#TRANSLATION_TEMPERATURE=0.3
#TRANSLATION_MAX_TOKENS=2048

# Connection settings
#TRANSLATION_CONNECTION_TIMEOUT=300
#TRANSLATION_CONNECTION_POOL_LIMIT=50

# Max input tokens as percentage of model context
#TRANSLATION_MAX_TOKEN_PERCENT=80

# =============================================================================
# Editor Toolkit Module
# =============================================================================

# Module-specific overrides (fall back to global LLM settings if unset)
#EDITOR_LLM_BACKEND=ollama
#EDITOR_OLLAMA_URL=http://localhost:11434
#EDITOR_VLLM_URL=http://localhost:8000
#EDITOR_DEFAULT_MODEL=gemma3:4b

# Maximum texts per batch request
#EDITOR_MAX_BATCH_SIZE=50

# LLM generation settings
#EDITOR_TEMPERATURE=0.3
#EDITOR_MAX_TOKENS=2048

# Connection settings
#EDITOR_CONNECTION_TIMEOUT=300
#EDITOR_CONNECTION_POOL_LIMIT=50

# Max input tokens as percentage of model context
#EDITOR_MAX_TOKEN_PERCENT=80

# =============================================================================
# Refinements (Session Management)
# =============================================================================

# Redis key prefix for refinement sessions
#REFINEMENT_KEY_PREFIX=refine

# Default session TTL in seconds (default: 2 hours)
#REFINEMENT_DEFAULT_TTL=7200

# Maximum refinement iterations per session
#REFINEMENT_MAX_ITERATIONS=10

# Maximum regenerations per session
#REFINEMENT_MAX_REGENERATIONS=5

# =============================================================================
# Logging
# =============================================================================

# Log output directory
#LOG_OUTPUT_DIR=./logs/output

# Maximum log file size in bytes (default: 10MB)
#LOG_MAX_BYTES=10485760

# Number of rotated backup files to keep
#LOG_BACKUP_COUNT=5

# Character preview length for prompts/responses in logs
#LOG_PREVIEW_LENGTH=200

# Log file names
#LOG_FILE_REQUESTS=llm_requests.log
#LOG_FILE_ERRORS=llm_errors.log
#LOG_FILE_METRICS=llm_metrics.log
#LOG_FILE_DEBUG=llm_debug.log

# =============================================================================
# Performance Tuning Guide
# =============================================================================
#
# IMPORTANT: If using Ollama, set OLLAMA_NUM_PARALLEL on the Ollama server.
# Without it, Ollama serializes all requests regardless of concurrency settings.
# Each parallel slot needs ~3-4GB RAM (CPU) or ~1-2GB VRAM (GPU) per model context.
#
# --- CPU-only systems (16-32GB RAM) ---
#   export OLLAMA_NUM_PARALLEL=2
#   ollama serve
#
#   VISION_MAX_CONCURRENT=2
#   SUMMARIZATION_MAP_CONCURRENT=2
#   SUMMARIZATION_REDUCE_CONCURRENT=2
#
# --- GPU systems (V100 / A100 with 32+ GB VRAM) ---
#   export OLLAMA_NUM_PARALLEL=4
#   ollama serve
#
#   VISION_MAX_CONCURRENT=4
#   SUMMARIZATION_MAP_CONCURRENT=4
#   SUMMARIZATION_REDUCE_CONCURRENT=3
#   SUMMARIZATION_MAX_WORDS_PER_BATCH=5000
#   SUMMARIZATION_MAX_CHUNKS_PER_BATCH=15
#
# NOTE: Vision and summarization run sequentially (never at the same time),
# so the max parallel Ollama load = max(VISION_MAX_CONCURRENT, SUMMARIZATION_MAP_CONCURRENT).
# OLLAMA_NUM_PARALLEL should be >= that value for true parallelism.
