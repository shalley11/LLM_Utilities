# =============================================================================
# LLM_Utilities - VLLM Configuration
# =============================================================================
#
# Hardware: 2x NVIDIA V100 32GB GPUs | dtype: float32 | max_tokens: 8192
#
# VRAM budget (float32):
#   Model weights:  ~48GB (12B × 4 bytes)
#   Available VRAM: ~60.8GB (64GB × 0.95 utilization)
#   KV cache room:  ~12.8GB → supports ~3 concurrent requests
#
# VLLM launch command:
#   python -m vllm.entrypoints.openai.api_server \
#     --model google/gemma-3-12b-it \
#     --tensor-parallel-size 2 \
#     --max-model-len 8192 \
#     --gpu-memory-utilization 0.95 \
#     --dtype float32 \
#     --port 8000
#
# Usage: cp .env-vllm .env
#

# =============================================================================
# Core LLM Backend
# =============================================================================

LLM_BACKEND=vllm
VLLM_URL=http://localhost:8000
DEFAULT_MODEL=google/gemma-3-12b-it
LLM_TEMPERATURE=0.3

# =============================================================================
# LLM Connection Settings
# =============================================================================

LLM_CONNECTION_TIMEOUT=600
LLM_CONNECTION_POOL_LIMIT=100
LLM_CONNECTION_POOL_LIMIT_PER_HOST=100

# =============================================================================
# Redis Configuration
# =============================================================================

REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REFINEMENT_TTL=7200

# =============================================================================
# Text Extractor
# =============================================================================

EXTRACTOR_UPLOAD_DIR=/tmp/text_extractor/uploads
EXTRACTOR_IMAGE_DIR=/tmp/text_extractor/images
EXTRACTOR_MAX_FILE_SIZE_MB=50
EXTRACTOR_MAX_PAGES=50

# =============================================================================
# Chunking Module
# =============================================================================

CHUNKING_LLM_BACKEND=vllm
CHUNKING_VLLM_URL=http://localhost:8000
CHUNKING_DEFAULT_MODEL=google/gemma-3-12b-it

CHUNKING_CONNECTION_TIMEOUT=1800
CHUNKING_CONNECTION_POOL_LIMIT=100

# Overlap between chunks (chars). 300 = ~2-3 sentences, good for dense scanned PDFs.
# With boundary-aware snapping, overlap starts at a clean sentence boundary.
CHUNKING_DEFAULT_OVERLAP=300
CHUNKING_DEFAULT_RESERVE_FOR_PROMPT=1000
CHUNKING_DEFAULT_PROCESS_IMAGES=true
CHUNKING_MIN_TEXT_LENGTH=10

# Context: 8192 tokens → chunk_size = (8192-1000)*0.5*4 = 14,384 chars
CHUNKING_DEFAULT_CONTEXT_LENGTH=8192
CHUNKING_CHARS_PER_TOKEN=4
CHUNKING_MAX_BATCH_SIZE=100

# =============================================================================
# Vision Processor (used by chunking for image descriptions)
# =============================================================================

VISION_LLM_BACKEND=vllm
VISION_MODEL=google/gemma-3-12b-it
VISION_VLLM_URL=http://localhost:8000

VISION_REQUEST_TIMEOUT=600
VISION_TEMPERATURE=0.3
VISION_MAX_TOKENS=500

# float32 leaves ~12.8GB for KV cache → max ~3 concurrent requests safely
VISION_MAX_CONCURRENT=3

# =============================================================================
# Summarization Module
# =============================================================================

SUMMARIZATION_LLM_BACKEND=vllm
SUMMARIZATION_VLLM_URL=http://localhost:8000
SUMMARIZATION_DEFAULT_MODEL=google/gemma-3-12b-it

SUMMARIZATION_DEFAULT_TYPE=detailed

# Batch limits — 3000 words ≈ 750 tokens input, fits well in 8192 context
SUMMARIZATION_MAX_WORDS_PER_BATCH=3000
SUMMARIZATION_MAX_CHUNKS_PER_BATCH=10

SUMMARIZATION_INTERMEDIATE_WORDS=300
SUMMARIZATION_FINAL_WORDS=500

SUMMARIZATION_TEMPERATURE=0.3
SUMMARIZATION_MAX_TOKENS=2048

# Concurrency — limited by float32 VRAM headroom (~3 concurrent max)
SUMMARIZATION_MAP_CONCURRENT=3
SUMMARIZATION_REDUCE_CONCURRENT=2

SUMMARIZATION_CONNECTION_TIMEOUT=600
SUMMARIZATION_CONNECTION_POOL_LIMIT=100

# Max input tokens: 8192 × 0.75 = 6144 tokens for input
SUMMARIZATION_MAX_TOKEN_PERCENT=75

# =============================================================================
# Translation Module
# =============================================================================

TRANSLATION_LLM_BACKEND=vllm
TRANSLATION_VLLM_URL=http://localhost:8000
TRANSLATION_DEFAULT_MODEL=google/gemma-3-12b-it
TRANSLATION_DEFAULT_TARGET_LANGUAGE=english
TRANSLATION_MAX_BATCH_SIZE=50
TRANSLATION_TEMPERATURE=0.3
TRANSLATION_MAX_TOKENS=2048
TRANSLATION_CONNECTION_TIMEOUT=600
TRANSLATION_CONNECTION_POOL_LIMIT=100
TRANSLATION_MAX_TOKEN_PERCENT=75

# =============================================================================
# Editor Toolkit Module
# =============================================================================

EDITOR_LLM_BACKEND=vllm
EDITOR_VLLM_URL=http://localhost:8000
EDITOR_DEFAULT_MODEL=google/gemma-3-12b-it
EDITOR_MAX_BATCH_SIZE=50
EDITOR_TEMPERATURE=0.3
EDITOR_MAX_TOKENS=2048
EDITOR_CONNECTION_TIMEOUT=600
EDITOR_CONNECTION_POOL_LIMIT=100
EDITOR_MAX_TOKEN_PERCENT=75

# =============================================================================
# Refinements (Session Management)
# =============================================================================

REFINEMENT_KEY_PREFIX=refine
REFINEMENT_DEFAULT_TTL=7200
REFINEMENT_MAX_ITERATIONS=10
REFINEMENT_MAX_REGENERATIONS=5

# =============================================================================
# Logging
# =============================================================================

LOG_OUTPUT_DIR=./logs/output
LOG_MAX_BYTES=10485760
LOG_BACKUP_COUNT=5
LOG_PREVIEW_LENGTH=200
LOG_FILE_REQUESTS=llm_requests.log
LOG_FILE_ERRORS=llm_errors.log
LOG_FILE_METRICS=llm_metrics.log
LOG_FILE_DEBUG=llm_debug.log
