# LLM_Utilities

A production-ready async Python API for document processing with LLM-powered text extraction, chunking, summarization, translation, and text editing.

## Features

- **Text Extraction**: Extract text from PDF, DOCX, DOC, TXT files with image handling
- **Chunking**: Boundary-aware chunking that splits at headers, paragraphs, and sentences with page-wise overlap
- **Vision Processing**: Image description via Vision model with repetition detection and output cleanup
- **Image Placeholders**: Numbered `[IMAGE_X]` placeholders survive through chunking and summarization pipelines
- **Summarization**: Hierarchical MAP-REDUCE summarization with parallel async batch processing
- **Translation**: Multi-language translation with batch support
- **Editor Toolkit**: Text editing (rephrase, professional, proofread, concise)
- **Async I/O**: Non-blocking LLM calls using `aiohttp` with shared session reuse across pipeline phases
- **Multi-Backend Support**: Ollama and VLLM backends with per-module configuration
- **Configurable Concurrency**: Semaphore-controlled parallel LLM calls (MAP/REDUCE/Vision) tunable via env vars
- **Context Length Protection**: Model-wise context limits with warnings and rejection
- **Refinement Cycle**: Iteratively improve outputs with user feedback via Redis sessions
- **Comprehensive Logging**: Structured logs with request tracing and metrics
- **Modular Architecture**: Each service has its own config and can use different models/backends

---

## Installation

```bash
pip install -r requirements.txt
```

### Prerequisites
- Python 3.8+
- Redis server running on `localhost:6379`
- Ollama running on `localhost:11434` (or VLLM on `localhost:8000`)

---

## Configuration

All settings are configurable via environment variables. Copy the example file and modify as needed:

```bash
cp .env.example .env
```

For VLLM deployments (e.g., V100 GPUs):
```bash
cp .env-vllm .env
```

Edit `.env` to override any defaults. See `.env.example` for the complete list of variables with descriptions. Module-specific variables (e.g., `CHUNKING_DEFAULT_MODEL`) override the global equivalents (e.g., `DEFAULT_MODEL`).

---

## Quick Start

### Run the API Server

**Using the startup script (recommended):**
```bash
./start.sh                    # Start with default settings
./start.sh --reload           # Start with auto-reload for development
./start.sh --port 9000        # Custom port
./start.sh --workers 4        # Multiple workers for production
./start.sh --help             # Show all options
```

**Using uvicorn directly:**
```bash
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

The startup script automatically checks prerequisites (Redis, Ollama) and provides helpful status messages.

### Access Documentation

- Swagger UI: http://localhost:8000/docs
- ReDoc: http://localhost:8000/redoc
- Download OpenAPI spec: http://localhost:8000/openapi-spec/download

### Offline Documentation

For air-gapped or offline environments, download the Swagger UI and ReDoc static assets before deployment:

```bash
./setup_docs.sh    # Downloads ~3-4 MB of static files (requires internet)
```

After running, `/docs` and `/redoc` will work without internet access.

---

## Project Structure

```
LLM_Utilities/
├── config.py                 # Global shared configuration
├── main.py                   # FastAPI application
├── schemas.py                # Core Pydantic models
├── requirements.txt          # Dependencies
├── start.sh                  # Service startup script
├── setup_docs.sh             # Download offline docs assets
├── .env.example              # Environment config template (Ollama defaults)
├── .env-vllm                 # Environment config for VLLM/V100 deployments
│
├── static/                   # Offline docs assets (generated by setup_docs.sh)
│   ├── swagger-ui/
│   ├── redoc/
│   └── favicon.png
│
├── LLM/                      # LLM client module
│   ├── config.py             # LLM client settings
│   ├── llm_client.py         # Async LLM backend (Ollama/VLLM)
│   └── prompts.py            # Core task prompts
│
├── text_extractor/           # Text extraction module
│   ├── config.py             # Extractor-specific settings
│   ├── service.py            # FastAPI endpoints
│   ├── extractor.py          # Core extraction logic
│   └── schemas.py            # Pydantic models
│
├── chunking/                 # Chunking module
│   ├── config.py             # Chunking and vision settings
│   ├── llm_client.py         # Module-specific LLM client
│   ├── service.py            # FastAPI endpoints
│   ├── chunker.py            # Core chunking logic (boundary-aware splitting)
│   ├── vision_processor.py   # Vision model for images (with output cleanup)
│   └── schemas.py            # Pydantic models
│
├── summarization/            # Summarization module
│   ├── config.py             # Summarization-specific settings
│   ├── llm_client.py         # Module-specific LLM client
│   ├── service.py            # FastAPI endpoints (pipeline orchestration)
│   ├── summarizer.py         # Hierarchical MAP-REDUCE summarization
│   ├── prompts.py            # Summary prompts (with image placeholder preservation)
│   └── schemas.py            # Pydantic models
│
├── translation/              # Translation module
│   ├── config.py             # Translation-specific settings
│   ├── llm_client.py         # Module-specific LLM client
│   ├── service.py            # FastAPI endpoints
│   ├── translator.py         # Core translation logic
│   ├── prompts.py            # Translation prompts
│   └── schemas.py            # Pydantic models
│
├── editortoolkit/            # Editor toolkit module
│   ├── config.py             # Editor-specific settings
│   ├── llm_client.py         # Module-specific LLM client
│   ├── service.py            # FastAPI endpoints
│   ├── editor.py             # Core editing logic
│   ├── prompts.py            # Editing prompts
│   └── schemas.py            # Pydantic models
│
├── refinements/              # Refinement session storage
│   └── refinement_store.py   # Redis session management
│
└── logs/                     # Log files (auto-created)
    └── logging_config.py     # Logging configuration
```

---

## API Endpoints

### Text Extraction

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/docAI/v1/extract/upload` | POST | Extract text from uploaded file |
| `/api/docAI/v1/extract/upload/simple` | POST | Simple extraction (markdown only) |
| `/api/docAI/v1/extract/path` | POST | Extract from server file path |
| `/api/docAI/v1/extract/cleanup/{document_id}` | DELETE | Cleanup extracted files |
| `/api/docAI/v1/extract/supported-types` | GET | List supported file types |

### Chunking

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/docAI/v1/chunk/process` | POST | Create chunks from markdown text |
| `/api/docAI/v1/chunk/text` | POST | Create chunks from plain text |
| `/api/docAI/v1/chunk/file` | POST | Create chunks from uploaded file |
| `/api/docAI/v1/chunk/config` | POST | Calculate optimal chunk configuration |
| `/api/docAI/v1/chunk/models` | GET | List supported models |

### Summarization

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/docAI/v1/summarize/chunks` | POST | Hierarchical summarization of chunks |
| `/api/docAI/v1/summarize/text` | POST | Summarize text with auto-chunking |
| `/api/docAI/v1/summarize/file` | POST | Summarize uploaded file (full pipeline) |
| `/api/docAI/v1/summarize/config` | GET | Get summarization config |
| `/api/docAI/v1/summarize/types` | GET | Get available summary types |

### Translation

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/docAI/v1/translate/text` | POST | Translate single text |
| `/api/docAI/v1/translate/batch` | POST | Translate multiple texts |
| `/api/docAI/v1/translate/config` | GET | Get translation config |

### Editor Toolkit

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/docAI/v1/editor/edit` | POST | Edit text (rephrase, professional, proofread, concise) |
| `/api/docAI/v1/editor/refine` | POST | Refine previous edit with feedback |
| `/api/docAI/v1/editor/batch` | POST | Batch edit multiple texts |
| `/api/docAI/v1/editor/tasks` | GET | List supported editing tasks |
| `/api/docAI/v1/editor/config` | GET | Get editor config |

### Refinements (Session Management)

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/docAI/v1/refinements/create` | POST | Create new refinement session |
| `/api/docAI/v1/refinements/get/{request_id}` | GET | Get session data |
| `/api/docAI/v1/refinements/update` | POST | Update session with refined result |
| `/api/docAI/v1/refinements/regenerate` | POST | Update session with regenerated result |
| `/api/docAI/v1/refinements/delete/{request_id}` | DELETE | Delete session |
| `/api/docAI/v1/refinements/extend` | POST | Extend session TTL |
| `/api/docAI/v1/refinements/status/{request_id}` | GET | Get session status |
| `/api/docAI/v1/refinements/config` | GET | Get refinement config |

### Core Processing

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/docAI/v1/process` | POST | Process text with task |
| `/api/docAI/v1/refine` | POST | Refine previous result |
| `/api/docAI/v1/regenerate` | POST | Regenerate from original |
| `/api/docAI/v1/status` | POST | Get session status |
| `/api/docAI/v1/delete` | POST | End session |
| `/api/docAI/v1/extend` | POST | Extend session TTL |
| `/health` | GET | Health check |

---

## Architecture

### Summarization Pipeline

The `/summarize/file` endpoint runs the full pipeline:

```
File Upload → Text Extraction → Chunking → Summarization → Post-Processing
                                   │              │               │
                            Vision model     MAP-REDUCE      Image placeholder
                            for images       parallel         injection
```

**MAP-REDUCE Summarization:**
1. **Batching**: Chunks are grouped by word count (`MAX_WORDS_PER_BATCH`) and chunk count (`MAX_CHUNKS_PER_BATCH`)
2. **MAP Phase**: Each batch is summarized in parallel with semaphore-controlled concurrency (`SUMMARIZATION_MAP_CONCURRENT`)
3. **REDUCE Phase**: Batch summaries are combined hierarchically (groups of 5) until they fit in one batch, up to 5 levels
4. **Final Combine**: Remaining summaries are combined using the user's chosen summary type prompt

All phases share a single `aiohttp.ClientSession` for HTTP connection reuse.

### Boundary-Aware Chunking

The chunker splits text at natural boundaries instead of arbitrary character positions. When a PDF page exceeds the chunk size limit, or when computing overlap between chunks, the system finds the nearest boundary in this priority order:

1. **Markdown headers** (`#`, `##`, `###`, etc.) — strongest semantic break
2. **Paragraph breaks** (double newline `\n\n`)
3. **Sentence endings** (`. `, `? `, `! `)
4. **Word boundaries** (whitespace) — weakest, but never splits mid-word

This ensures chunks contain complete thoughts and overlap regions start at readable boundaries.

```
Without boundary-aware splitting:
  Chunk 1: "...the patient was diagnosed with diabe"
  Chunk 2: "tes mellitus type 2..."

With boundary-aware splitting:
  Chunk 1: "...the patient was diagnosed with diabetes mellitus type 2."
  Chunk 2: "The treatment plan includes..."
```

### Image Placeholder Pipeline

Images in documents are processed through a multi-stage pipeline:

1. **Vision Processing**: Each image is described by the Vision model (with repetition loop detection)
2. **Placeholder Assignment**: Images get numbered placeholders (`[IMAGE_1]`, `[IMAGE_2]`, etc.) with descriptions embedded in chunk text
3. **Prompt Preservation**: All summarization prompts instruct the LLM to preserve `[IMAGE_X]` placeholders
4. **Post-Processing**: After summarization, `inject_image_placeholders()` normalizes mangled variants and fuzzy-matches missing placeholders to relevant sentences

---

## Configuration Reference

### Global Configuration (`config.py`)

```python
LLM_BACKEND = "ollama"          # or "vllm"
OLLAMA_URL = "http://localhost:11434"
VLLM_URL = "http://localhost:8000"
DEFAULT_MODEL = "gemma3:4b"
LLM_TEMPERATURE = 0.3

MODEL_CONTEXT_LENGTHS = {
    "gemma3:4b": 8192,
    "gemma3:12b": 8192,
}
```

### Chunking (`chunking/config.py`)

| Variable | Default | Description |
|----------|---------|-------------|
| `CHUNKING_LLM_BACKEND` | `ollama` | Backend type (falls back to `LLM_BACKEND`) |
| `CHUNKING_DEFAULT_MODEL` | `gemma3:4b` | Model for chunk size calculation |
| `CHUNKING_DEFAULT_OVERLAP` | `200` | Overlap between chunks in characters |
| `CHUNKING_DEFAULT_RESERVE_FOR_PROMPT` | `1000` | Tokens reserved for LLM prompt |
| `CHUNKING_DEFAULT_PROCESS_IMAGES` | `true` | Process images with Vision model |
| `CHUNKING_MIN_TEXT_LENGTH` | `10` | Minimum text length to create a chunk |
| `CHUNKING_DEFAULT_CONTEXT_LENGTH` | `8192` | Default context for unknown models |
| `CHUNKING_CHARS_PER_TOKEN` | `4` | Characters per token estimate |
| `CHUNKING_MAX_BATCH_SIZE` | `100` | Maximum chunks per batch |
| `CHUNKING_CONNECTION_TIMEOUT` | `1800` | HTTP timeout in seconds |

**Chunk size formula**: `(context_length - reserve_for_prompt) * 0.5 * chars_per_token`

Example: `(8192 - 1000) * 0.5 * 4 = 14,384 characters`

### Vision Processor (`chunking/config.py`)

| Variable | Default | Description |
|----------|---------|-------------|
| `VISION_LLM_BACKEND` | `ollama` | Backend (falls back to chunking, then global) |
| `VISION_MODEL` | `gemma3:4b` | Vision model for image processing |
| `VISION_REQUEST_TIMEOUT` | `600` | Request timeout in seconds |
| `VISION_MAX_CONCURRENT` | `2` | Max concurrent vision requests |
| `VISION_TEMPERATURE` | `0.3` | Vision model temperature |
| `VISION_MAX_TOKENS` | `500` | Max tokens for vision output |

### Summarization (`summarization/config.py`)

| Variable | Default | Description |
|----------|---------|-------------|
| `SUMMARIZATION_LLM_BACKEND` | `ollama` | Backend type |
| `SUMMARIZATION_DEFAULT_MODEL` | `gemma3:4b` | Model for summarization |
| `SUMMARIZATION_DEFAULT_TYPE` | `detailed` | Default summary type |
| `SUMMARIZATION_MAX_WORDS_PER_BATCH` | `3000` | Max words per MAP batch |
| `SUMMARIZATION_MAX_CHUNKS_PER_BATCH` | `10` | Max chunks per MAP batch |
| `SUMMARIZATION_INTERMEDIATE_WORDS` | `300` | Target words for batch summaries |
| `SUMMARIZATION_FINAL_WORDS` | `500` | Target words for final summary |
| `SUMMARIZATION_TEMPERATURE` | `0.3` | LLM temperature |
| `SUMMARIZATION_MAX_TOKENS` | `2048` | Max tokens for LLM output |
| `SUMMARIZATION_MAP_CONCURRENT` | `2` | Max parallel LLM calls in MAP phase |
| `SUMMARIZATION_REDUCE_CONCURRENT` | `2` | Max parallel LLM calls in REDUCE phase |
| `SUMMARIZATION_CONNECTION_TIMEOUT` | `1800` | HTTP timeout in seconds |
| `SUMMARIZATION_MAX_TOKEN_PERCENT` | `80` | Max input tokens as % of context |

### Translation (`translation/config.py`)

| Variable | Default | Description |
|----------|---------|-------------|
| `TRANSLATION_LLM_BACKEND` | `ollama` | Backend type |
| `TRANSLATION_DEFAULT_MODEL` | `gemma3:4b` | Model for translation |
| `TRANSLATION_DEFAULT_TARGET_LANGUAGE` | `english` | Default target language |
| `TRANSLATION_MAX_BATCH_SIZE` | `50` | Max texts per batch |
| `TRANSLATION_TEMPERATURE` | `0.3` | LLM temperature |
| `TRANSLATION_MAX_TOKENS` | `2048` | Max tokens for LLM output |
| `TRANSLATION_CONNECTION_TIMEOUT` | `300` | HTTP timeout |
| `TRANSLATION_MAX_TOKEN_PERCENT` | `80` | Max input tokens as % of context |

### Editor Toolkit (`editortoolkit/config.py`)

| Variable | Default | Description |
|----------|---------|-------------|
| `EDITOR_LLM_BACKEND` | `ollama` | Backend type |
| `EDITOR_DEFAULT_MODEL` | `gemma3:4b` | Model for editing |
| `EDITOR_MAX_BATCH_SIZE` | `50` | Max texts per batch |
| `EDITOR_TEMPERATURE` | `0.3` | LLM temperature |
| `EDITOR_MAX_TOKENS` | `2048` | Max tokens for LLM output |
| `EDITOR_CONNECTION_TIMEOUT` | `300` | HTTP timeout |
| `EDITOR_MAX_TOKEN_PERCENT` | `80` | Max input tokens as % of context |

### Text Extractor (`text_extractor/config.py`)

| Variable | Default | Description |
|----------|---------|-------------|
| `EXTRACTOR_UPLOAD_DIR` | `/tmp/text_extractor/uploads` | Temp directory for uploads |
| `EXTRACTOR_IMAGE_DIR` | `/tmp/text_extractor/images` | Temp directory for images |
| `EXTRACTOR_MAX_FILE_SIZE_MB` | `50` | Max file size in MB |
| `EXTRACTOR_MAX_PAGES` | `50` | Max pages per document |

### Refinements

| Variable | Default | Description |
|----------|---------|-------------|
| `REFINEMENT_KEY_PREFIX` | `refine` | Redis key prefix |
| `REFINEMENT_DEFAULT_TTL` | `7200` | Session TTL in seconds (2 hours) |
| `REFINEMENT_MAX_ITERATIONS` | `10` | Max refinements per session |
| `REFINEMENT_MAX_REGENERATIONS` | `5` | Max regenerations per session |

### Redis

| Variable | Default | Description |
|----------|---------|-------------|
| `REDIS_HOST` | `localhost` | Redis server host |
| `REDIS_PORT` | `6379` | Redis server port |
| `REDIS_DB` | `0` | Redis database number |

### Logging

| Variable | Default | Description |
|----------|---------|-------------|
| `LOG_OUTPUT_DIR` | `./logs/output` | Log files directory |
| `LOG_MAX_BYTES` | `10485760` | Max log file size (10MB) |
| `LOG_BACKUP_COUNT` | `5` | Rotated backup files to keep |
| `LOG_PREVIEW_LENGTH` | `200` | Preview length in logs |

---

## Performance Tuning

### CPU-Only Systems (16-32GB RAM)

Use conservative concurrency to avoid memory exhaustion:

```bash
# Ollama: set server-side parallelism
export OLLAMA_NUM_PARALLEL=2
ollama serve

# App concurrency settings
VISION_MAX_CONCURRENT=2
SUMMARIZATION_MAP_CONCURRENT=2
SUMMARIZATION_REDUCE_CONCURRENT=2
```

### GPU Systems (V100 / A100)

Higher concurrency is possible with GPU VRAM headroom:

```bash
# Ollama with GPU
export OLLAMA_NUM_PARALLEL=4
ollama serve

# App concurrency settings
VISION_MAX_CONCURRENT=4
SUMMARIZATION_MAP_CONCURRENT=4
SUMMARIZATION_REDUCE_CONCURRENT=3
SUMMARIZATION_MAX_WORDS_PER_BATCH=5000
SUMMARIZATION_MAX_CHUNKS_PER_BATCH=15
```

### VLLM Deployment (2x V100 32GB, float32)

For production VLLM with tensor parallelism:

```bash
# Launch VLLM server
python -m vllm.entrypoints.openai.api_server \
  --model google/gemma-3-12b-it \
  --tensor-parallel-size 2 \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.95 \
  --dtype float32 \
  --port 8000

# Use the VLLM env template
cp .env-vllm .env
```

VRAM budget (float32): ~48GB model weights + ~12.8GB KV cache room = max 3 concurrent requests.

### Concurrency Notes

- Vision and summarization run **sequentially** (never at the same time) in the file pipeline
- Max parallel Ollama load = `max(VISION_MAX_CONCURRENT, SUMMARIZATION_MAP_CONCURRENT)`
- `OLLAMA_NUM_PARALLEL` should be >= that value for true parallelism
- Each Ollama parallel slot needs ~3-4GB RAM (CPU) or ~1-2GB VRAM (GPU)
- VLLM uses PagedAttention with a shared memory pool (more efficient than Ollama's per-slot allocation)

---

## API Examples

### Translation

```bash
curl -X POST http://localhost:8000/api/docAI/v1/translate/text \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Hello, how are you?",
    "target_language": "spanish",
    "user_id": "user123"
  }'
```

**Response:**
```json
{
  "request_id": "uuid-here",
  "original_text": "Hello, how are you?",
  "translated_text": "Hola, ¿cómo estás?",
  "source_language": "auto-detected",
  "target_language": "spanish",
  "model": "gemma3:4b",
  "status": "completed"
}
```

### Editor Toolkit

```bash
curl -X POST http://localhost:8000/api/docAI/v1/editor/edit \
  -H "Content-Type: application/json" \
  -d '{
    "text": "i think we should probably maybe consider this",
    "task": "professional",
    "user_id": "user123"
  }'
```

**Response:**
```json
{
  "request_id": "uuid-here",
  "original_text": "i think we should probably maybe consider this",
  "edited_text": "We should consider this matter.",
  "task": "professional",
  "model": "gemma3:4b",
  "status": "completed"
}
```

### Summarization

```bash
curl -X POST http://localhost:8000/api/docAI/v1/summarize/text \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Your long document text here...",
    "summary_type": "brief",
    "user_id": "user123"
  }'
```

### File Summarization (Full Pipeline)

```bash
curl -X POST http://localhost:8000/api/docAI/v1/summarize/file \
  -F "file=@document.pdf" \
  -F "summary_type=detailed" \
  -F "process_images=true" \
  -F "user_id=user123"
```

**Response:**
```json
{
  "request_id": "uuid-here",
  "summary": "The document covers... [IMAGE_1] ...key findings include...",
  "summary_type": "detailed",
  "method": "hierarchical",
  "total_chunks": 25,
  "total_words": 12000,
  "batches": 4,
  "levels": 2,
  "model": "gemma3:4b",
  "images": {"IMAGE_1": "/path/to/image.png"}
}
```

---

## Request Tracking

All endpoints support `request_id` for tracking:

- **Fresh request**: If `request_id` is not provided, a new UUID is generated
- **Subsequent call**: If `request_id` is provided, it's used for tracking the chain of requests

```json
{
  "request_id": "existing-uuid",
  "text": "...",
  "task": "..."
}
```

---

## Editor Tasks

| Task | Description |
|------|-------------|
| `rephrase` | Improve clarity, readability, and remove repetitions |
| `professional` | Rewrite in formal business tone |
| `proofread` | Fix grammar, spelling, punctuation |
| `concise` | Shorten text, remove unnecessary words |

---

## Summary Types

| Type | Description |
|------|-------------|
| `brief` | Short summary capturing core idea |
| `detailed` | Comprehensive summary covering all points |
| `bulletwise` | Bullet-point summary |
| `executive` | High-level summary for leadership |

---

## Guardrails

The API includes built-in guardrails to prevent processing failures and ensure reliable operation.

### Document Page Limit

Documents exceeding **50 pages** are rejected during extraction.

**Error Response:**
```json
{
  "detail": {
    "error": "document_too_long",
    "message": "Document has 75 pages which exceeds the maximum allowed limit of 50 pages.",
    "total_pages": 75,
    "max_pages": 50,
    "suggestion": "Please split the document into smaller parts (max 50 pages each) and process them separately."
  }
}
```

### Token Limit

Text exceeding the configured percentage of model context length is rejected before processing.

**Error Response:**
```json
{
  "detail": {
    "error": "token_limit_exceeded",
    "message": "Text contains approximately 8,500 tokens which exceeds the maximum allowed limit of 6,553 tokens.",
    "estimated_tokens": 8500,
    "max_tokens": 6553,
    "model": "gemma3:4b",
    "model_context_length": 8192,
    "usage_percent": 129.72,
    "suggestion": "Please reduce the text length or split into smaller chunks for processing."
  }
}
```

### Guardrail Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `EXTRACTOR_MAX_PAGES` | 50 | Maximum pages for document extraction |
| `EXTRACTOR_MAX_FILE_SIZE_MB` | 50 | Maximum file size in MB |
| `TRANSLATION_MAX_TOKEN_PERCENT` | 80 | Max input tokens as % of context |
| `EDITOR_MAX_TOKEN_PERCENT` | 80 | Max input tokens as % of context |
| `SUMMARIZATION_MAX_TOKEN_PERCENT` | 80 | Max input tokens as % of context |

---

## Error Handling

| HTTP Code | Reason |
|-----------|--------|
| 400 | Bad request / Document too long / Token limit exceeded |
| 404 | Request ID not found or expired |
| 500 | Internal server error |

---

## Logging

Log files in `logs/` directory:

| File | Content |
|------|---------|
| `llm_requests.log` | All LLM API calls |
| `llm_errors.log` | Error-level logs |
| `llm_metrics.log` | Performance metrics (JSON) |
| `llm_debug.log` | Detailed debug info |

---

## Version History

| Version | Changes |
|---------|---------|
| 3.3.0 | Parallel async MAP-REDUCE summarization, configurable concurrency, session reuse, VLLM deployment config, image placeholder pipeline with fuzzy matching, vision output cleanup |
| 3.2.0 | Boundary-aware chunking: splits at headers/paragraphs/sentences, no mid-word overlap |
| 3.1.0 | Guardrails: page limit (50), token limit validation across all services |
| 3.0.0 | Modular architecture, translation, editor toolkit, module-specific configs |
| 2.5.0 | Text extraction, chunking, summarization modules |
| 2.2.0 | Async I/O with aiohttp, connection pooling |
| 2.0.0 | Refinement cycle with Redis storage |
| 1.0.0 | Initial release |
